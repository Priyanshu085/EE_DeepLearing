{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data\n",
    "text_data = \"\"\"\n",
    "            Deep learning is a subfield of artificial intelligence that focuses on developing algorithms that can learn and make predictions from data. \n",
    "            It has gained immense popularity in recent years due to its ability to solve complex problems in various domains such as computer vision, \n",
    "            natural language processing, and robotics.\n",
    "            \"\"\"\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text_data])\n",
    "encoded_text = tokenizer.texts_to_sequences([text_data])[0]\n",
    "\n",
    "# Generate training sequences\n",
    "sequences = []\n",
    "for i in range(1, len(encoded_text)):\n",
    "    sequence = encoded_text[:i+1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "# Split sequences into input and output\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n",
    "\n",
    "# Generate new text\n",
    "seed_text = \"deep learning\"\n",
    "for _ in range(10):\n",
    "    encoded_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    encoded_seed = pad_sequences([encoded_seed], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted_word_index = np.argmax(model.predict(encoded_seed), axis=-1)\n",
    "    predicted_word = [word for word, index in tokenizer.word_index.items() if index == predicted_word_index][0]\n",
    "    seed_text += \" \" + predicted_word\n",
    "\n",
    "print(\"Generated Text:\", seed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
